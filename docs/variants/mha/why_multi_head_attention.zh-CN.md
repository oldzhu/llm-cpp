> [English](why_multi_head_attention.md)

# 为什么多头注意力重要（不只是并行）

> 说明：本中文版本翻译进行中；完整内容请先阅读英文版。

摘要：多头注意力允许不同的子空间/模式并行建模，能提升表达能力与训练稳定性；它不是单纯的“更快”。
