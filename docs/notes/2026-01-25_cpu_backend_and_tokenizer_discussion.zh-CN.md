# 2026-01-25：CPU 后端验证与分词器/中文嵌入讨论

## CPU 后端：训练/推理再验证
- Debug 模式下重新编译并运行 `ctest`：全部测试通过（1/1）。
- 快速冒烟测试：在 `the-verdict.txt` 上训练 60 步，保存检查点，然后用该检查点生成文本。损失平滑下降，生成正常（仅 60 步时输出仍较乱，属预期现象）。
- 用该检查点对数据集做“下一个字节” sanity 检查：5 次中有 2 次 top-1 命中；步数少时属于正常（步数/模型变大后会提升）。
- 运行方式见 `README.md`，参数实现见 `src/main.cpp`。

## 字节级嵌入能否表示中文？
- 可以：如果输入为 UTF-8，字节级 vocab=256 可以表示中文，因为中文字符本质上是 UTF-8 字节序列（通常每字 3 字节）。
- 但字节级对中文（及所有非 ASCII）效率低：
  - 序列更长（每字符多个 token），模型需更长上下文和更多计算。
  - 训练早期更易生成非法 UTF-8 字节，输出易乱码，需 `--escape-bytes 1` 或后处理。
- 所以“不能嵌入中文”不准确——能，但只是粗糙基线，对中文语料质量低/收敛慢。

## 下一步：从字节切换到 BPE（或类似方案）
- GPT-2 风格的字节回退 BPE 是很好的升级：仍支持任意 Unicode（含中文），但通过合并高频字节序列减少 token 数。
- 如需实现，可做极简“只加载”分词器（不训练）：
  - 保留当前字节分词器作兜底，
  - 加载 BPE 词表/合并规则，
  - 相应调整 `vocab_size` 和检查点格式。
- SentencePiece Unigram 也是中文友好选项。
